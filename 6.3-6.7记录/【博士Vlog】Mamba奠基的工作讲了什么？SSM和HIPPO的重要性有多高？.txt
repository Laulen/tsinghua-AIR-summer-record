【博士Vlog】Mamba奠基的工作讲了什么？SSM和HIPPO的重要性有多高？
https://www.bilibili.com/video/BV1vF4m1F7KG/

00:00 大家好
00:00 我是PHTVLOG
00:01 那么今天呢就给大家讲一下曼巴之前的工作
00:04 这个工作是efficient model
00:06 in long sequence with s s s这样一个模型
00:09 所以这个它有四个
00:11 所以这个模型被称为S4这样一个模型
00:13 那这个文章的作者呢我们来看一下
00:15 这是他是albert go
00:16 你看这也是albert go
00:17 所以说呢这个人呢是一直在做这样的一个研究
00:19 然后他是这个斯坦福的这样的一个诶
00:22 他怎么变了
00:23 变到另外一个学校去了
00:24 你看这个文章里面
00:25 他还是啊stanford university
00:27 那么在这个文章里
00:27 他已经换了一个学校对吧
00:29 那么我们来看一看
00:30 就说这个这样的一个文章
00:32 他是一个做了一个什么样的研究
00:33 那么这个文章是讲的曼巴的一个前身
00:36 就是曼巴这个文章呢
00:37 它不是一个凭空蹦出来的这样一个东西
00:39 它其实是经过几次很长时间的演化
00:41 最后才做出来这样一个研究
00:43 所以说呢斑马是这样的一个情况
00:45 那我们来看一看这个这个文章呢
00:47 是非常难懂的一篇文章
00:48 所以说呢如果我们想把这个文章呢搞明白的话
00:50 那我们就必须借助一些AI的力量对吧
00:54 我们要把这个文章进行一个总结
00:56 看一看这个文章到底讲些什么东西
00:57 这个文章呢和我的领域也没有关系
00:59 所以说呢我讲这个文章呢
01:01 就是纯粹的就是去了解一下
01:03 新的这样的一个知识
01:04 有没有什么样的一个新的
01:05 一样样的一个研究方向
01:07 但是呢这个方向呢
01:08 可能和我也没有什么太大的关系
01:09 所以说呢我呢隔行如隔山嘛
01:11 所以我也不太懂
01:12 所以我呢就把它用拆GPT去解释一下
01:15 这样我就能够比较有效的去看明白
01:17 这个文章到底在讲些什么东西
01:18 而拆GPT呢还会做一些事
01:20 比如说脑补等
01:21 会给大家讲一下什么叫脑补
01:22 那脑补的话就可以让我们有更多的知识
01:25 因为他他是去读过很多的文章的
01:27 那么我们可能没有读过很多的文章
01:29 对一个新的领域不了解
01:30 但拆JPG呢是可以有这样的记忆能力的
01:33 那么我们来看看
01:33 拆GBT是怎么去讲这些文章的呢
01:35 他说这个文章呢叫这个名字
01:36 作者是这几个人对吧
01:38 然后呢ela cool是一直在做的
01:40 所以ela cool呢他在YOUTUBE上是讲过视频的
01:42 在之前的啊
01:43 我记得YOUTUBE上有人去专访斯坦福
01:45 还是去专专访这个普林斯顿还是怎么样的
01:48 反正是去专访的时候呢
01:50 elbe good就讲了一下
01:51 他这个模型是怎么去训练的
01:53 怎么就这模型怎么回事
01:55 它里面做了很多动图
01:56 所以你可以去好好看一下那个视频
01:58 大概一个小时
01:59 因为他这个东西呢和我的这个领域呢不是很像
02:01 所以说呢我就没有去特别仔细的去了解
02:03 他这个具体是怎么做的
02:05 我们就是大概的去看一看他的文章就可以了
02:08 那么这个文章提出了一种新的
02:09 这样的一个序列模型
02:10 就是这样一个SSSS这样模型
02:12 所以简称S它有四个S
02:14 他呢就比较好去处理一个长期带关系
02:16 就是LRD这样一个关系
02:18 长期带关系
02:18 就是说我们有一个序列它特别的长对吧
02:21 那我们这个特别长的一个序列的话
02:23 我们能不能找一个序列之间的这样一个联系呢
02:25 那这个东西非常的有用
02:26 为什么呢
02:27 比如说我们处理音频信号的时候
02:28 它每秒钟采样是44100次对吧
02:30 那它是一个非常非常长的一个序列
02:32 所以说你看现在生成音乐的这个网络呢
02:35 它不是很好
02:35 它有两个原因
02:36 一个是因为音乐的这个这个内容呢
02:38 他有些版权的问题
02:39 所以说他为了解决版权的问题呢
02:41 他就必须得有一大堆的这个事要去解决
02:44 所以说呢他就没法有效地生成一个好的音乐
02:47 他用的这个音乐库呢
02:48 可能都是一些不太好的音乐
02:49 所以它生成出来的结果就不是特别的好
02:51 那第二个可以用来处理这个长期依赖关系的呢
02:54 就是比如说DNA序列啊
02:55 RNA序列这些序列都非常非常长对吧
02:58 那这些序列可以去做一个长期的关系
03:00 还有比如说像SA这样的模型去生成一个视频
03:03 那么师生是一个视频的话
03:04 也是一个长期的关系
03:05 所以SA呢应该是用的传说嘛这样的一个东西
03:08 那它呢也可以用上以后用上这个模型啊
03:11 solo的话
03:11 就是说他在修正中加上了一个transformer
03:13 这样的话它就可以把这个序列的比较长的
03:16 有效的去处理出来
03:17 大概是这样的一个意思吧
03:18 那么呢他就讲了
03:19 就说传统的模型有什么呢
03:21 比如说有这样的模型
03:22 有CN这样的模型
03:23 还有传former这样的模型
03:24 但是三个不同的这样的一个列
03:25 那么GRU和IOSTM是RN的后续
03:28 所以说呢GRU和ISTM是RN的这样一个
03:32 族的这样一部分
03:34 然后呢他们这些方法都有专门的变体
03:36 来捕获这种长期依赖
03:37 但是他们处理超过1万步以上的
03:38 非常长的序列的时候
03:40 就非常的困难了
03:41 他们就没法处理一个很长的序列
03:43 他们要么就是TENTION不够
03:44 要么就是他比如这个序列
03:45 它会顺梯度的消失或梯度爆炸
03:47 因为它是一个成性的关系对吧
03:49 那这个这个东西它本来就不是用来处理长的
03:51 这样一个关系
03:52 它本来就是做一个局部感受
03:53 也本来就是处理图像的
03:54 那你让他去处理这样的序列的话就有些困难
03:57 transformer呢它呢空间占的太大了
04:00 它是一个N方的关系
04:01 所以说呢它的长期的
04:02 他要是出了一个非常长的序列的话
04:04 它的那个矩阵就变得特别巨大
04:06 然后它的计算速度非常
04:08 因为我说了
04:08 他是一个矩阵嘛
04:09 他就找你们的tension
04:10 它是用一个矩阵来找
04:11 所以他就非常非常难卖
04:12 买四就没有这个问题
04:13 S呢还可以比之前的方法
04:15 进行一个更高效的一个计算
04:16 那就是说S之前还有其他的方法啊
04:19 其他方法的话也是使用这SSM这个方法
04:21 但是呢他呢数学数学上的这个推导呢
04:24 可能没有做得那么深入
04:25 所以说它的方法它的效果就没有那么好
04:28 它不仅他可以做一个比这些方法更好的一个
04:31 高效的计算
04:32 它呢还有一个理论上的优势
04:33 那这个理论上的优势的话
04:34 就是它的效果比这个方法要好一点
04:37 然后在四中最重要的一个矩阵呢是这个A矩阵
04:40 他把A矩阵呢做一个低质校正
04:42 所以说可以把A矩阵呢给它拍平
04:43 拍成一个一维的
04:44 这样的话就可以让这个S呢就是稳定的对角化
04:48 之后呢
04:48 就可以让S这个这个方法
04:50 它变成一个一维的一个方法
04:51 它就可以让它的计算量变得非常小
04:53 然后呢它是它的空间占用率也变得非常的有效
04:56 空间占用也非常小
04:57 给它实现了在不同的基准测试之中的
04:59 一个比较好的一个结果
05:01 那具体的应用上面就是S呢
05:02 对于一个不同长度的长度
05:04 这个序列的表现的非常出色对吧
05:06 下面有进行数据增强或者辅助损失的情况下
05:08 它就能达到91%的标准
05:09 在seventeen这个数据集上
05:11 seventeen是什么飞机啊
05:13 然后我记得我记得有一个类是飞机
05:15 但是其他类我不记得
05:16 但是就一共只有十个类对吧
05:18 然后它里面有各种各样的飞机
05:19 所以还有三分100
05:20 三分100
05:20 就是把十个类的飞机呢再给他
05:22 再把每一类的飞机再分成十类
05:24 比如战斗机
05:25 民航机之类的
05:26 但是他没有他没有
05:27 他没有给出那个标签标签的具体的含义了
05:30 他应该是有的
05:31 在网站上有
05:31 但是我就是从来都是只用0~9这样的标签
05:34 所以我也不知道它具体是什么含义
05:35 但是它就是有十类对吧
05:37 十类再细分成100类这个样子
05:39 它达到91%的
05:40 这样一个准确率是非常高的
05:41 那么它与较大的突击RSNE
05:43 其实能达到一个相似的这样的一个水平
05:45 所以说呢他的这个水平呢还是非常高的
05:47 那么呢它这个其他的任务上
05:49 也大大缩小与传SER之间的差距
05:51 但是呢他的差距不是很大
05:53 但是他的这个速度呢却比这个模型快了60倍
05:56 所以它是一个非常有效的一个模型
05:58 所以说他这个文章的展示
05:59 S作为一个呃非常非常大的一个潜力吧
06:02 那么我们后来看到了曼巴这个work
06:05 那么我们来说一下
06:06 为什么这个传former这个模型叫做变压器
06:08 而不叫变形金刚呢
06:09 就是说这是个变压器
06:10 它有个初级绕组
06:11 它有一个次级绕组
06:12 但也可能是反过来
06:13 也可能这是初级
06:14 这是四级
06:14 但是我们就不用在乎这件事了
06:16 还有一个初级绕组
06:17 有一个次级绕组
06:18 那么它就可以把电压呢去升高或者降低
06:21 这样这样的一个工作
06:22 那么传送门也是这样
06:24 他输入的时候呢
06:25 这个地方你看他这个输入他这个这个地方呢
06:27 它的这个编码器对吧
06:28 这是他的初级绕组啊
06:30 这个地方是它的解码器
06:31 是它的次级绕组
06:32 然后初级绕组和次级绕组呢
06:33 它有一个不同的任务
06:34 比如说初级绕组呢
06:35 是可以进行一个序列的匹配啊
06:37 然后进行一个翻译啊这样一个操作
06:39 然后次级绕组呢是它可以进行一个比较
06:42 有比较好的涌现能力
06:43 他有很好的生存能力
06:45 所以说呢他初级绕组部分呢被谷歌摘走了
06:47 变成了一个birds
06:48 然后次级绕组呢被GPT也被open i摘走了
06:51 变成了一个GPT
06:51 所以bot呢他呢比较好的去做一个
06:54 比如说一个MING这样的工作
06:56 就是MING这样的工作
06:57 就是他可以去比较好去做一个对应
06:59 或者是做一个翻译啊这样的工作
07:02 然后JPT呢他就可以比较好去做一个生成啊
07:05 这样的工作
07:06 所以说如果要是不知道的话
07:07 我们可以跑到这个拆GP上去问一下
07:10 就是啊我问我自己对吧
07:12 问问他到底是什么样的一个情况
07:14 然后boy和JP都是从传说中各自取了一部分
07:17 问问他为什么前途不同呢
07:18 然后他就说哦
07:19 这两部分都是基于transformer的这样一个架构
07:21 但他们设计的哲学和他的目标不太一样
07:24 比如说BT有什么它的优势和劣势呢
07:26 优势呢就是它可以进行一个双向的上下文理解
07:28 所以他可以做翻译嘛对吧
07:29 他有强大的微调能力
07:30 还有丰富的语义表示啊
07:32 就是它可以生成的词汇句子
07:34 短语和句子它的质量比较高
07:36 但是它有几个问题
07:37 一个问题是他的训练开销呢它比较大
07:39 他的这个训练的比较慢
07:40 所以说这个模型呢没法扩展的很大
07:42 但是它需要一个精心的设计
07:44 也就是说如果是我像拆GPT
07:46 g p t two到g p t three
07:47 这样我就把它强行扩大100倍
07:49 那么他可能就会崩掉了
07:51 所以说呢GBT呢这方面就会有一些优势
07:54 那么JPG的优势和劣势是什么呢
07:55 就是它有很强的生存能力
07:57 因为它是解码器嘛对吧
07:58 然后呢他的训练的比较简单
08:00 然后他的迁移能力非常强
08:01 因为它的解码器嘛
08:02 所以他迁移能力很强嘛
08:03 然后它的劣势呢就是说它的上下文是单向的
08:06 所以说他没法做翻译的时候
08:08 效果可能会稍微差一点
08:09 然后还需要用更长的上下文窗口
08:10 才能够去解决问题
08:12 那么这个会导致计算量的增加
08:13 那么它的路径是不一样的
08:15 他最开始的时候
08:16 因为它是2017年
08:16 2018年的时候的模型
08:18 2017年的时候出了传村嘛
08:19 那这两个东西应该是
08:20 2018年或2019年的时候出来的
08:22 那个时候大家还没看清楚
08:24 这个生成是这个AI的这个能力
08:26 所以当时波尔特好像对胜一筹
08:27 但之后呢大家就是比如拆GBT对吧
08:30 当然拆GBT是很往后的事
08:32 还有diffusion这样的模型
08:33 大家会发现就说用AI去生成一个东西
08:36 是个非常厉害的事情
08:37 所以说呢到后面的它这个目标变了
08:40 所以说呢谷歌和open结果呢也发生了变化
08:42 open i先做做最新的东西对吧
08:44 做的都是set of2次的东西
08:46 然后呢胡歌呢现在就很略略落后了一筹
08:49 然后你看open air现在做的那个生成视频
08:52 这个东西就是说他不搞
08:54 所以他一搞就一鸣惊人
08:55 但其他人还在调这种三秒钟五秒钟的视频
08:58 想让他生成的非常的想要生成的效果更高
09:01 100%分之二的时候
09:02 SORA这个模型已经生成了几十秒的视频了啊
09:05 所以说open a这个公司还是蛮厉害的一个公司
09:07 那我们来看看这个文章的这样一个摘要吧
09:09 那么这个摘要呢其实已经被拆JPG
09:11 大概的给他整个的总结了一遍了
09:13 来看看这个文章到底讲些什么事呢
09:15 就说有很多的模型
09:16 比如说RNCN
09:17 还有传说这样的模型
09:18 他都可以去捕获这个长期依赖性
09:19 在他们捕获到一个很长的长度的时候
09:21 就会出现一个问题
09:22 所以他们提出一个方法
09:23 这个方法是XT等于AXT加上BUT
09:27 那么这个地方呢它的公式呢
09:29 你看谷歌翻译呢翻译的时候少了个撇对吧
09:31 这个地方是它的导数
09:32 所以说呢这个是它的整个的公式
09:34 那么这个A这个部分呢
09:36 就是它的这个迭代迭代的这个因数
09:38 所以A这个参数呢非常重要
09:40 因为你看这个是XDJX撇T
09:41 所以它的迭代因数是非常重要的
09:43 所以A呢需要仔细的选择
09:45 但B呢就是一个B呢就稍微弱一点对吧
09:48 他这个四个参数里最重要的可能是D这个参数
09:50 然后C这个参数是决定输出的强度是多少
09:53 所以C呢也还是挺重要
09:54 所以ABC这三个参数比较重要
09:56 最重要的是A这个参数
09:57 所以说呢他这个文章呢就着重去影响
09:59 研究了一下这个A这个参数是怎么样的
10:01 所以说呢这个A这个参数呢
10:04 是使用hp这个矩阵做到的
10:05 然后呢他就做到了一个S4这样一个模型
10:08 然后他又想着
10:09 因为A这个参数呢
10:10 他hp矩阵是二维的
10:11 所以说他想个办法用低质校正来调节A
10:14 就可以让A呢就更小一点对吧
10:16 就是让它把它把它拍平
10:17 这样的话相当于这个A呢
10:19 就相当于变成了一个一维一样的东西
10:21 所以说他这个空间占用率呢
10:23 就比之前要是不像是它还是二维的话
10:26 不跟transformer一样的吗
10:27 他就比穿梭门要小
10:28 大概是这样
10:29 然后最后他做了一个什么样的结果
10:31 就是说他其实没有辅助运算的情况下
10:33 它的效果非常的好
10:34 对不对
10:34 还有各种各样的任务上都非常的好
10:36 然后他说他有一个基本任务
10:37 他有什么基本任务呢
10:38 叫做pass x任务
10:39 什么pass x轴有两个点
10:41 然后呢我去找这两个点之间一个路径
10:43 我们一会给大家看一下pass x是什么样的任务啊
10:45 我们有一个图
10:46 然后呢在别的模型上面呢
10:48 他们都不太好对吧
10:49 这个模型呢它不仅速度快
10:50 而且它呢还能够做
10:52 就是这个上面表现的比较好
10:54 所以讲了这么一件事
10:55 那么简介呢讲的是差不多的一样的一个内容
10:58 就是他说了说有各种各样的模型
11:00 这简介的前半段讲的就是说CNRNCTM
11:03 还有传former
11:04 它的结果是怎么样的啊
11:06 他就他就是包括了很多对抗
11:08 什么梯度消失啊之类的
11:09 做了很多这样的工作
11:10 这样的工作呢可以让他们有一个更强的一个
11:12 更长时间的一个记忆
11:14 但是呢在LDL的任务上呢
11:16 还是表现的不是特别好
11:18 但是他们这个模型呢
11:19 在LDL这个上面表现的就非常非常的不错了
11:21 那么我们来看一看
11:22 就说他这个模型到底是什么样的东西呢
11:24 SSM是个什么东西呢
11:25 如果我们有一个输入
11:27 对不对
11:27 在这是在自动信号处
11:29 不在在自动控制原理之中的一张图
11:31 那有个信号进来对不对
11:33 那是什么控制器
11:34 就是这个这个东西
11:35 它有个控制器
11:36 有受控对象
11:36 然后这边呢是有一个反馈对吧
11:38 就是这样一个图
11:39 那么底下就是它的模型
11:40 你看这是X的这样的一个导数
11:43 然后呢等于等于ax加上bu
11:45 然后Y等于CX加上DU
11:46 然后这个A就在这四个矩阵里面
11:49 你看这个Y呢就是用来输出的
11:50 所以说呢他就是X的这样的一个倍数的关系
11:54 所以呢C呢是一个还有点重要性的一个东西吧
11:57 D呢就几乎没用
11:58 然后D就是纯粹的就是一个偏执对吧
12:00 就是让他这个信号变高一点还是变低一点
12:03 它就是纯粹的一个偏执
12:04 这没什么用
12:05 然后呢在这两个里面呢
12:06 B呢可能还有点用
12:07 因为它可以控制一下
12:08 就说反馈的这样的一个强度
12:10 然后A呢是最强的
12:12 因为它是乘直接乘在X上的
12:14 所以这个A呢要调的太大的话
12:16 可能会出现爆炸
12:17 因为它是来回来去的
12:19 转它会爆炸
12:20 如果是A调太小的话
12:21 它可能会梯度可能会消失
12:23 所以说呢就是A这个参数是最重要的
12:25 所以他这张图就来解释
12:28 所以A呢是用一个什么hip的
12:29 这样一个矩阵就选出来的
12:31 然后最后一个图呢就是说他怎么把这个work呢
12:33 在这个空间上高速计算
12:35 因为我们显卡是离散的
12:36 计算机
12:36 只能计算离散的东西
12:37 那么他怎么把这个东西给它
12:38 变成一个离散化的这样一个过程呢
12:40 那么它使用了一个CN的方式
12:42 把它最后做一个计算
12:43 它的速度比较快一点
12:45 那么这个图呢就是整个的这个样子
12:47 那么这个图呢我们不是很懂它到底讲什么
12:49 所以怎么办呢
12:50 把这个图整个塞到拆P去讲一下就可以了
12:52 所以说在拆GP里他是这么回答的
12:54 就是说左侧呢
12:55 它是讲的这四个矩阵是怎么回事对吧
12:57 引化方程是怎么回事
12:58 怎么输出的
12:58 然后中间的是讲的
12:59 就是说啊
13:00 如何如何去做这个LDL这个长期依赖性对吧
13:03 他是讲了一下这件事情
13:05 然后右侧是讲话怎么进行快速离散计算
13:07 计算
13:07 计算的方式是使用这个卷积计算机的输出操作
13:10 所以说它转为现在显卡
13:13 现在的这些GPU呢
13:14 它其实都是通过卷积
13:15 它都是在卷积上面进行一个深度设计
13:17 就是说最开始的任务就是从ALEX到后面的VJ
13:21 J到后面的rice night dance night
13:23 然后什么efficient night night night
13:25 还有什么啊
13:25 我们都讲过了对吧
13:26 我们讲过好多好多东西
13:28 然后还有什么呃呃什么inception exception对吧
13:32 Inception v2
13:33 这些东西他们都是卷积的这样的一个操作
13:35 那么所以现在现在显卡呢就是老黄那边显卡呢
13:39 他做的时候都是按照这个
13:41 都是按照这个卷积来进行优化
13:43 所以说呢如果你要能使用卷积核的话
13:44 它会有一个非常快的速度对啊
13:46 所以说呢他呢就是使用卷积这个方式
13:48 进行了这样的一个加速
13:50 但是我们或者说曼巴他用不到卷积啊
13:52 所以说他们使用一个新的方法来做
13:54 我们你如果感兴趣的话
13:55 可以去看看曼巴那个文章
13:56 为什么它不能使用卷积
13:57 因为他要要把中间的部分给扔掉
13:59 所以他没法使用卷积进行一个有效的提速
14:02 来讲这样一件事情
14:03 我们来看看它这个部分
14:05 那这个部分其实讲的是一样的
14:06 就这个地方讲的就是说D这个参数呢没什么用
14:09 所以D这个参数他直接扔了
14:10 D这个参数不管直接扔掉
14:11 因为这个D呢就是一个偏执
14:13 偏执没用对吧
14:14 然后呢C这个参数呢稍微再调调
14:16 可能简单调调
14:17 B这个参数也稍微调调是吧
14:18 A这个参数是最重要的
14:19 所以他分析一下哪个参数是最重要的
14:21 哪个参数应该他们着重去进行一个调参的
14:24 那么A1个参数是怎么调的呢
14:25 因为他就是使用了一个叫people的
14:26 这样的一个矩阵对吧
14:27 这个矩阵这个矩阵是二维的
14:28 矩阵
14:29 和这个矩阵去进行一个调参的
14:31 这样一个过程对吧
14:32 他这个矩阵呢非常好
14:33 为什么他可以把这个60%的这样
14:34 一个结果变成98%对吧
14:36 他可以增加这个效果
14:37 就讲了KO这个矩阵的这样一个操作
14:39 那么ho这个矩阵啊
14:41 第32.3
14:42 这个部分呢
14:42 讲的就是离散时间的SSM的这样一个循环
14:45 表示
14:45 就是他怎么把它变成一个离散的这样一个状态
14:48 那么这都是纯数学的东西
14:49 我们也不用太仔细看
14:51 因为我们真正要不要在我这里用这个模型的话
14:54 就是把人家写好的模型GITHUB代表拿过来
14:56 直接跑一下
14:57 抄一下对吧
14:58 比如我们用clip
14:59 我也不会去劝一下clip对吧
15:01 我只要clip那个代码拿过来对吧
15:03 我clip我也讲过对吧
15:04 你可以去看一下clip
15:05 那我可能直接把他的那个文本编码器
15:07 或者它的图像编码器直接拿过来
15:08 我直接用
15:09 用完就完事
15:10 就做这样一个word
15:11 那就是它整个的这样一个过程
15:13 那你就自己去看一下他是怎么做的
15:15 他怎么训练的呀
15:16 然后呢这个有一个问题是这样的
15:19 就是说在这个里面它可以用卷积来进行训练
15:21 但是呢在mama这篇文章
15:22 这是ma的一个图
15:23 那么他做一个是从这个地方复制到这个地方
15:26 它中间是没有断的
15:27 你看这砖是没有断的
15:28 他在SSM上就可以使用这个
15:30 这个可能是使用CN这样的方式去做
15:32 但是MANA呢他做了一个selective copy
15:34 那么这个selective copy呢
15:35 就让他比如说我有个句子对吧
15:37 在推特上面我有一句话对吧
15:38 这句话里面讲了一个什么什么的事情
15:40 然后我想开GP来提取一下
15:42 就是说它中间有什么这个重要的内容呢
15:45 他可能这个画的中间有些废话
15:47 这些废词就被他扔掉了
15:48 都会从中提取到一些非常重要的一个信息
15:51 然后把它copy出来
15:51 变成一个完整的句子
15:53 那么对于这样一个语言模型中
15:54 非常重要的一个任务呢
15:56 他呢在transformer里非常容易执行
15:58 因为transformer呢它是一个二维的矩阵对吧
15:59 他说你们直接选就完事了
16:01 那这个曼巴这样模型呢他就没法做
16:03 那么它呢就要使用那些算法
16:05 把这个问题解决掉了
16:07 然后呢
16:07 判长也就只能摒弃掉这个CN这样一个模型了
16:09 那我们之前讲过他为什么会这样去做
16:12 那么我们来讲更深层的一个东西吧
16:14 这个东西呢就是说在这个ABCD这四个里面
16:16 A是最重要
16:17 白A是个二维的
16:18 那它是二维的话呢就会比较麻烦
16:20 所以他怎么办呢
16:21 大概就使用了这么一个方法
16:22 NPL这个方法啊
16:24 把这个矩阵呢给他压压缩了一下
16:26 把它变得小一点
16:27 那这个方法你看这个方法就很像是呃
16:29 这有点像是SVD的这样一个算法
16:32 把这个方法把这个真的给他压低了一点
16:34 把他的这个他其实实际上这个矩阵呢
16:37 它是一个它这个hip这个矩阵呢
16:39 它其实是一个rank并不是很高的一个矩阵
16:41 我觉得他的这个隐性的rank并不是很高
16:44 所以说可以把它搞得低一点吧
16:46 不用搞得像一个二维的那么大
16:48 当然是这样一个情况吧
16:49 然后呢这个地方就是说在不同的模型之间呢
16:52 它有一个时间复杂度的这样一个情况
16:54 比如说这个第一个是convolution对吧
16:56 conclusion是卷积的
16:57 然后这个是循环神经网络
16:59 就是RN
17:00 然后这个是attention
17:01 就是传说嘛
17:02 这是他们的这个模型
17:03 但有几个参数来对比
17:04 第一个是parameters i ameters对吧
17:06 就是参数量
17:07 然后是春天的速度
17:08 然后是空间占用率
17:09 还有一个是并行
17:10 是否会并行
17:11 然后还有一个就是这个in inference
17:13 就是他的这个最后做一个测试
17:15 都做一个推理这样的一个情况
17:17 那么我们就能看到他们之间的不同的
17:19 时间复杂度是怎么样的
17:20 你看PARALAN的话呢
17:21 这个RN是没法进行了一个有效的这样一个
17:24 并行计算的
17:25 那在之前的文章中呢
17:26 在之前那个讲那个外卖那个文章的时候
17:28 我给大家详细的理了一下
17:29 可以去看一下那期视频
17:30 那么它在不同的地方它有不同的这样一个速度
17:33 然后呢这个图呢我们要是看不太懂的话呢
17:36 就把它塞拆JBT里面去问一下
17:38 你看拆JP前面一部分和我之前说的一样对吧
17:40 就讲了一下
17:41 这个这个图中的这个这个表的横向啊
17:44 还有纵向它这些坐标是什么意思对吧
17:46 底下这个讲的就是说啊在不同的这个里面呢
17:49 每一个字母的含义
17:50 那我之前是把这篇文章发给拆GBT看过了
17:53 所以拆GBT呢
17:54 石头顶的这个字母呢
17:55 都有一个比较好的一个理解对吧
17:57 比如说呢就是说H啊
17:58 B是什么意思呀
18:00 L是什么意思呀对吧
18:01 那最后呢就是说S这个模型
18:03 它具有一个比较好的计算效率
18:04 那么这个拆解BT提取出来的一个结果
18:06 并且它只是并行处理
18:08 并且他的推理的时候的复杂度非常的低
18:10 你看这个经你看这个地方就错了
18:13 对不对
18:13 他推理的时候复杂度是LH的平方
18:15 那这个是
18:16 这个是这个明显是一个CN的这样的一个情况吧
18:19 他的情况是H方
18:21 所以拆GBT说话
18:22 你不能完全信
18:23 大概你看看他那个意思就行了
18:25 尤其是对于你对于你没用的文章
18:27 你看看它的意思就行了
18:28 对于你自己有用的文章呢
18:29 最好还是自己亲自去看一下
18:31 不然的话你搞错了
18:33 你在最后写的时候出现一个问题就很麻烦了
18:36 他就拆JP有时候会犯傻
18:37 比如说他你让他给你修改一下你的那个
18:39 比如说修改一下你的这个呃abstract
18:42 让它看着更像就是更有美观的这种语言
18:45 习惯一点
18:45 他最后给你来一个说呃
18:47 Sure here is your abstract
18:48 然后冒号后面是你的abstract
18:50 然后有些人呢就直接把这整个段子往上一粘贴
18:53 然后一粘贴之后的结果就是就很尴尬
18:56 这个文章就有的文章发表出去了
18:58 然后一看第一句话是sure here is your abstract
19:01 对啊所以说拆机器用的时候要特别小心
19:03 讲了这么一件事
19:04 你看啊在这个里面的parameters
19:06 这个地方是L乘以H差的地方是H方
19:08 其实呢它呢和CN的差距也不是特别大
19:11 然后这个地方是最重要的
19:12 你看这个地方都有各种各样的方
19:14 对这个地方它没有方对吧
19:15 它只是多加了一个值
19:17 所以说它和CN呢是比较像的
19:19 然后他的space和CN也是一样
19:21 它的parallel也是可以
19:22 parallel它的音音音fence
19:24 他在做推理的时候呢
19:25 它比CN的计算量呢其实还小一点啊
19:27 所以说这个模型是综合了这三个东西的
19:30 一个大成的优点
19:31 然后呢最后得到了一个非常好的一个结果
19:33 然后他们到后面就开始做实验了对吧
19:36 他做了很多的实验
19:37 那么他做的实验
19:38 比如说第一个实验呢
19:39 是说他和transformer去进行一个测试
19:41 那我要打败transformer
19:42 我就要和transformer去做一个对比嘛
19:44 对不对
19:44 第二节就是验证它的这个L
19:46 LRA的这样一个情况
19:47 这个部分我都没讲
19:48 因为这部分呢就是没有图
19:50 所以说就不讲了
19:51 第三个问题
19:51 就是说他是不是可以做一个通用的这样
19:53 一个模型呢
19:54 就是说它对于像啊文本啊
19:56 还有什么音乐啊这样的东西
19:57 它是一个好的预测呢
19:58 第一件事就是说它有个效率基准
20:00 看看它的效率是什么样的一个情况
20:02 它的速度是什么样的情况
20:04 那么这是它的速度这样一个情况
20:06 那么这个呃呃我们来看一下它的这个情况
20:09 就是这是四的这样一个模型对吧
20:10 你看他的这个这个充电的速度是很快的
20:14 但是别的模型的二三十倍的样子
20:16 然后它的这个内存的占用率是非常小的
20:18 它大概是别人的四1%的这个样子
20:20 那这张图表中呢就是他这个传错了这个模型
20:23 然后这个是另外一个模型
20:24 这是另外一个模型
20:25 就是四
20:25 你看四这个模型的速度是非常快的
20:28 然后呢他的这个空间占用是比较小的
20:31 然后它的speed呢在更大的范围上是更快的
20:34 所以你看啊这个这个state的这个模型呢
20:36 他呢在一个比较小的这样的图片上面的
20:39 它的表现呢就是烟雾八倍的这样的一个效果
20:42 然后呢它的这个memory是0.43倍的一个效果
20:44 它不是非常明显
20:45 你看到了吗
20:46 它又好了1.5倍
20:47 这个呢就是原来的一半对吧
20:49 再加一个大的模型
20:50 比如说它长度是4096的这样一个模型上的
20:52 它的这个速度就会快很多
20:54 然后它的memory也会小很多
20:56 这就是由于这个平方时间复杂度
20:58 和第一次时间复杂度这样的一个区别
21:00 越到后面呢它的这个差距就会变得越大
21:03 我们这部分讲完了
21:04 然后呢我们就可以把这个文章呢塞给拆GP
21:07 让拆GP去看一下
21:09 但是呢这个地方呢你可以暂停看一下
21:12 但是呢这个地方呢不是很有用
21:13 我就不去讲了
21:14 他这个具体的一些对比呢
21:17 我们只要知道它比别的模型好就行了
21:19 我们并不知道它比别的模型好多少
21:21 这个是文章的作者
21:22 还有就是审稿人必要去care的事情
21:24 我们不用去特别仔细的care这件事情
21:26 然后呢
21:27 这个就是说我们这个之前说那个啊
21:29 pass x这样一个任务是什么样的一个任务呢
21:32 什么是LIA呢
21:33 就是它有一个它有一个模型
21:34 有没有一个长期的这样一个思维能力的
21:37 就是我有两个点
21:37 对不对
21:38 我想在这两个点之间找到一条线
21:39 这条线的话这两个点连在一起
21:41 那么这条线就应该是大概是这样一条线
21:43 这条线就可以把这两个点连在一起
21:45 对不对
21:46 那么它里面有好多好多的线啊
21:48 那如果你要是个模型
21:49 没有一个很强的一个逻辑推理能力的话
21:52 他就没办法去进行一个有效的一个呃
21:54 结果吧就是一个是注意力比较长
21:57 还有一个就是有效的逻辑推理能力
21:58 那么呢这个呢这个图讲的什么呢
22:01 其实我也不是特别明白
22:02 所以我们就可以把它塞到拆GP里面去问一下
22:05 他讲出左侧呢
22:06 这是一个pass x任务这一个例子对吧
22:08 然后这个例子就是告诉你这个路径的设备连接
22:11 那我们要想知道路径设备连接呢
22:13 就只能有一个很强的这样的一个空间检索能力
22:16 那么右侧呢讲的什么
22:17 就是说S这个模型的最上面一层是什么样的
22:20 然后最下面一层呢是什么样子的
22:22 它把它们都给它进行一个可视化这样一个结果
22:24 所以我们能看到的上面这个图呢
22:26 就是说它有一个他的学习的东西呢不是特别多
22:29 他没有学习到很多的这样一个特征
22:31 下面这个图呢就学习到了一个比较高维度的
22:33 这样的一个特征对吧
22:34 所以说下面
22:34 所以说这个模型呢
22:36 就有一个比较强的这样一个学习能力
22:37 然后呢
22:38 看一下这个他学习到的一个
22:39 什么样的一个东西呢对吧
22:40 这是他学习的这样一个东西
22:42 然后呢我们就来对比一下不同的这个这个模型
22:46 你看这些模型呢他们都没有pass x这样一个成绩
22:49 他的成绩还是很高的对吧
22:51 然后image的成绩也很高对吧
22:53 这些成绩都很高
22:54 所以他最后的结果呢还是不错的
22:56 比这些模型都有一个很好的一个进步吧
22:58 然后呢这个地方呢就是去讲
23:00 他在一个通用的这样的一个呃啊
23:03 这这这张图应该是讲他和别的模型的一个
23:06 也是一个和别的模型的对比吧
23:08 反正就是比别的模型的要好
23:10 还没讲到通用的那个部分
23:11 比别的模型的要好
23:12 然后呢这个部分呢就是去讲
23:14 就是啊也是它的一个对比吧
23:17 就是它速度啊
23:19 还有他的image
23:20 它它的速度啊
23:21 还有bias啊这样的一个对比
23:23 bias并不重要
23:24 就是它有没有一个对比速度
23:26 一个对比
23:26 你看它的速度是比较快的一个情况
23:28 pa到后面呢他研究另外一件事
23:30 就是hp这个矩阵呢
23:31 对于这个模型到底有多大的影响力呢
23:33 他说他要做一个这样的一个这个影响嘛对吧
23:36 所以说他去做了一下这个hero这个模型的影响力
23:39 所以他做了一个消融实验
23:40 这个消融实验呢
23:41 一会讲一下消融实验是什么意思嘛
23:43 然后他就说啊
23:44 ho这个这个东西我们初始化的时候呢
23:46 他们的这个新的这个hip矩阵呢
23:48 他是不是有一个他做这个hip矩阵的
23:50 因为他这个矩阵不是hip是A矩阵嘛
23:52 AP主要用hip矩阵的话
23:54 他是不是有一个比较好的一个结果呢
23:56 他研究几个问题
23:57 hp矩阵的初始化有多重要呢
23:58 hp上训练SSM有多重要呢对吧
24:01 在没有hp的情况下会有什么样的结果呢对吧
24:03 他做了这样一个研究
24:04 所以说呢呃消融实验呢
24:06 就是说把这个一部分内容呢给它剔除掉
24:09 剔除掉之后呢
24:10 我们就可以研究一下这部分被扔掉了之后
24:12 它对于整个模型会有一个什么样的一个影响
24:15 对吧
24:15 就是消防事件这样一个具体的含义
24:17 那么可以去看一下
24:18 消融实验是什么样的一个意思
24:20 那这个你也可以到维基百科上去查
24:21 可能会比拆GBT说的更准一点
24:23 但是呢大概就是这样一个意思吧
24:26 那么消除实验
24:26 我们可以想到的就是shift这样一个模型
24:28 那shift这个模型的话
24:29 他就是说我们把某一个这个东西拿出来对吧
24:33 我们就和和没有这个东西的
24:35 就是说有这个东西和没做这个东西进行对比
24:37 对比之后的结果呢
24:38 我们可以画出来每一个feature importance
24:41 那么它其实也算是一种消融实验吧
24:43 但他用了一个博弈论的方法
24:44 更复杂的一个方法
24:45 所以它可以给我们介绍一下
24:47 就是说这些东西它的重要性是怎么样的
24:49 对我们来看看
24:51 他做做消融实验是什么样的一个情况
24:53 那么他做hip矩阵呢
24:54 你看这是他们自己发明的那玩意儿对吧
24:56 这hip矩阵
24:57 那hip结果呢也是在这里面最强的
25:00 然后这个是frozen a
25:01 就是说把这个A呢给它frozen
25:02 frozen a的结果呢和这个不frozen a的结果呢
25:06 他还是有点差距的对吧
25:08 A这个矩阵呢最好还是一个灵活的
25:09 可以变动的
25:10 可以在这个变动的这样一个矩阵
25:12 这样才会有一个比较好的一个结果
25:13 在春季赛的这个上面呢
25:15 你来看啊
25:16 就说他每一个分赛都能达到百分之百的这样
25:18 一个准确率
25:19 那肯定的呀
25:20 那春季赛他他就在上面优化了
25:22 越优化当然是越小了对吧
25:23 那我们看这个优化
25:25 我们可以看在这张图上
25:27 我们可以得到一个什么样的信息呢
25:28 就是说它优化速度是怎么样的
25:29 一般有hp矩阵的时候
25:30 它优化速度是比较快的
25:31 比较快就扛过去了
25:32 比较快就收敛了
25:33 然后呢这个不用这个HIPHOP的
25:35 比如说用random这个方法
25:36 他就收集的非常慢
25:37 所以他这个初始化的这个方法呢就非常好
25:40 非常大的影响了
25:41 就是说这个这个东西的训练
25:43 然后在这边呢你看他testing这个set上的
25:46 invitation set上
25:47 或者在testing set上呢
25:48 它的这个结果呢就是非常能说明问题的
25:50 就是说它能够说明
25:52 就说你这个矩阵
25:53 它最后有一个什么样的一个能力对吧
25:55 我们在进行一个不断的优化的时候
25:57 它会有一个能力嘛
25:58 对不对
25:59 所以你看这个hp这个矩阵呢
26:00 它的这个最后的能力是比较高的
26:02 就是它最后能达到一个比较高的天花板
26:04 这个样子
26:04 然后你看这个呢
26:05 这些在hero上好像就没有太多的这种
26:08 or fting的这样的一个问题
26:09 你看这个这个就只有OVERFITTING的问题
26:11 他到后面的时候越要他他最开始训练吧
26:14 他就在在提升准确率
26:16 然后到这个地方的时候达到一个最好的效果
26:18 然后之后呢模型呢就整天大脑去学
26:20 5年高考3年模拟
26:21 他就大脑就陷入到那个那个死循环之中了
26:23 知道吧
26:24 整天大脑就知道在学什么
26:25 5年高考3年模拟
26:26 他不知道什么叫高考题了
26:27 他整天到晚他拿到一个高考题的
26:29 结果就是哦5年高考3年模拟上没有这道题
26:31 我不会做这道题
26:32 所以到后面的时候
26:33 他就做了一个OVERFITTING
26:35 OVERFITTING的话效果就变得稍微差了一点
26:37 大概就是这样的一个意思吧
26:38 这个图都是这样的一个意思吧
26:40 好那么今天呢我就给大家捋了一下
26:43 这个ma这篇文章之前的这样的一个work
26:45 这个work呢就给大家讲一下
26:47 就是他的这个里面的这个基本的机理
26:49 是怎么样的
26:49 是怎么样去做的
26:50 因为后面慢慢铺平了这样一个道路
26:52 那我们今天的视频呢就讲到这里了
26:54 感谢大家的观看
26:55 我们下期再见
