当前最火爆文章：MambaOut 讲了什么？Mamba到底有没有用？
https://www.bilibili.com/video/BV1gy411Y7xa/

00:09 大家好
00:10 我是PHDV道
00:11 想一个10万粉丝的牌子呀
00:13 希望大家能够去多多的关注
00:15 那么之前呢在群里呢有很多人说呢
00:17 应该讲讲M8out
00:18 所以今天呢就给大家讲讲M8out
00:20 这是我们群里的投票最高的一个文章
00:22 所以说呢mamba out做了件什么事呢
00:24 其实非常的简单
00:25 他的问题呢就是说我们真的需要MANA
00:28 来做我们这样一个图像的一个任务吗
00:30 所以说呢他呢就把这个man
00:31 把这个主要内容就是SSM这个模块
00:34 从这个里面的给扔掉
00:35 你看这个它就是它的模型
00:36 它把这个SSM扔掉
00:38 唉
00:38 只保留了它里面的这个其他的这些部分
00:40 比如说这个convolution啊
00:41 这个部分
00:42 然后呢他又去做了一下这个实际的结果
00:45 发现呢V班霸呀
00:46 还有曼巴out呢
00:47 你看曼巴out的效果就比微班霸的要好一些
00:50 所以V麦霸呢有可能就是这个man霸呢
00:52 并没有起到一个什么实际上的一个作用
00:55 哎mea out呢把这个曼A扔掉了之后呢
00:57 效果呢依然是非常不错的
00:58 所以说呢他就是问一个问题
01:00 就MANA这个东西到底有没有用呢
01:02 那么他最开始呢给大家纪念了一下这个科比
01:05 布莱恩特对吧
01:06 就是what can i say
01:07 曼巴奥
01:07 就是这个科比呢在2016年夺冠了之后呢
01:10 他最后说的一句话
01:11 当然了
01:12 我不是特别关心体育啊
01:13 所以说这个问题呢只能问问拆JBT的
01:15 OK那么呢这就是map out对吧
01:17 他就是最开始的时候
01:18 你看他放了一个科比的这样的一个这个东西
01:21 然后呢
01:22 我们来看一看这个科比这个部分是怎么回事
01:24 因为我完全不知道我这个对体育不关心好
01:27 那就问问这个拆解器说卖8out什么意思
01:30 以及就是科比当时坠机的这样一个描述
01:32 是怎么样的
01:33 那么呢他就说呢是已故的这个篮球
01:36 这个明星对吧
01:37 一个传奇叫科比布莱恩特
01:39 他的这个职业生涯比赛中最后一场说的一句话
01:42 就是他彻底的就告别篮球场
01:44 是2016年4月13号
01:46 他退役之前的最后一刻
01:47 他呢在这个场上呢应该得了很多分数
01:50 然后带领的这个洛杉矶的湖人队
01:52 战胜了这个犹他的爵士队之后呢
01:54 就发表了一个演讲
01:55 最后呢就说了一句话
01:56 叫做啊这个曼巴out对吧
01:58 就他自己就是曼巴
02:00 因为他呢
02:01 这个我觉得他长得其实也很像曼巴是吧
02:04 所以说他就是像那个毒蛇嘛
02:06 所以他
02:10 所以说他就被叫做这个黑曼巴这样一个情况
02:13 那么呢科比呢是已故的一个篮球的传奇了
02:16 那非常不幸的
02:17 他在这个2020年1月26号的时候呢
02:20 他呢和他的这个女儿呢一起乘坐这个直升机
02:23 结果呢这个直升机呢发生了这个事故啊
02:26 就导致了他们就是就是敌难了
02:29 好那他这个当天的时候呢
02:31 这个气这个情况呢不太好对吧
02:33 这个遇到的浓雾的天气
02:35 这个能见度非常低
02:37 所以说呢整个的就这个机就机毁人亡了啊
02:41 那么这个整个的这个飞行
02:42 这个直升机上的九个人呢全部都遇难了
02:45 就是这样的一个情况
02:47 OK那这个事情之后呢
02:48 大家呢就都很悲痛
02:49 然后呢大家就开始去纪念这件事情
02:51 那这篇文章呢也算是一个纪念的文章吧
02:53 好的
02:54 那我们来说一说
02:55 MANA这个这个东西到底是怎么做的
02:57 MANA呢就是SSMSSM呢
02:59 就是我们所说的这样的一个
03:00 状态机的这样一个模型
03:02 他呢就是把自动控制原理之中的
03:04 这样一个概念呢搬过来
03:05 然后呢用这个概念呢来做了一个新的模型出来
03:07 它越像自动控制原理之中的一个
03:09 反馈回路的这样的一个模式啊
03:11 以前我给大家讲过
03:12 你可以去看一看之前曼巴那个文章是怎么讲的
03:15 然后呢他就是这样的
03:16 以为这样一个流
03:17 然后这边呢去做一些选择呀之类的
03:18 然后他呢又去做了一些内存的调整
03:20 它的速度就非常的快
03:22 麦霸呢现在已经成为了一个工具
03:23 很多的文章呢都在用man霸
03:25 但是麦霸呢就是你直接就是整个库
03:27 然后往里面一掉就可以了
03:28 并不是需要你自己去有一个
03:29 特别深入的一个了解好的
03:31 那我们来看一看
03:32 MANA这个东西做的什么比较合适呢
03:34 他就做一些比如说预测序列啊
03:35 这的模型的它会非常合适
03:37 但他做一些啊做一些比如说图像任务啊之类
03:40 它可能就不是特别合适
03:41 那序列预测呢就是说它是一个长的一个序列
03:43 那么你呢在预测的时候呢对吧
03:46 就是班霸
03:46 他是一个具有一个长的序列的一个预测能力
03:49 他呢能够记住过去的每一个状态是什么样
03:52 但他呢又不像LSTM和RN他忘的那么快
03:54 对吧
03:55 妈妈他的这个记忆能力呢是非常好的
03:57 因为他呢是通过一个自动控制原理
03:59 生成这样一个概念
03:59 但它效果非常好
04:00 它呢非常适合去做这样的一个啊
04:03 序列的一个预测
04:04 但是呢做图像的任任务呢
04:05 他可能就不是特别好
04:06 OK那么他虽然说之后提出了
04:08 比如说VM呀
04:09 就微战曼巴呀
04:10 还有VMANA
04:11 VMANA呢就是有点像这个是VM的结果
04:14 那V曼巴呢
04:14 它的这个结果呢
04:15 就是很像是我们所说的swing transformer
04:18 我们给大家看看swing transformer就是这个样子
04:20 那spin transformer呢和我们的VIV妈呢
04:22 长得就非常的像
04:23 是不是啊
04:24 这是呃思维传ANSFORMER和V麦骂这样一个特点
04:27 他那就是每一层变得越来越小
04:29 他就是在模仿CN嘛
04:30 大概就做这么个内容
04:31 但现在呢不仅有这个文源版
04:33 还有什么rise vivm
04:34 就是你看这个网络一出来之后呢
04:36 就开始各种各样的排列组合了
04:38 那这也是个排列组合
04:39 怎么排列组合呢
04:40 就是说呢我的以前是v man吧对吧
04:42 然后我现在给他在外面加上一个这个
04:44 一个残差块
04:45 那就是做这么一个啊一个网络
04:48 那这个都是排列组合组合出来的
04:50 那这个残差这个类
04:51 这个以前呢有这个swing transformer对吧
04:53 还有rice swin transformer
04:55 那这个现在也有rise vim吧
04:56 所以它有各种各样的排列组合的方式啊
04:59 就可以把各种各样的这些啊
05:00 模式都排列组合起来
05:01 然后效果呢可能会比之前稍微好一点
05:03 然后一个文章呢就这么水出来了
05:05 好的
05:05 那么呢我们说的外卖有个什么样的好处呢
05:08 首先呢第一件事它的速度比较快一点是吧
05:10 它的速度非常快
05:11 这件事就它内存占的非常小
05:13 所以说呢当别人的内存都崩了的时候
05:15 它的内存呢还能正常运行
05:16 这就是mama的这样一个优点
05:18 那么呢我们说mama呢它做一个图片序列的时候呢
05:21 就会产生一些问题
05:22 因为呢我们的mama呢是用来预测一个序列的
05:24 那预测一个序列的话呢
05:26 他呢就得把这个图片搞成一个序列
05:28 对不对
05:28 那这个序列你怎么搞呢
05:29 你是这么搞呢
05:31 还是这么搞呢
05:32 还是说就像一个就是像工字型的这样去搞呢
05:35 对吧
05:36 他这是你一个图片
05:37 它是输入是一张图片
05:39 你这个图片它不能给它搞成序列呀
05:40 你这个图片他这个搞成序列的话
05:43 有时候就不对了诶
05:44 所以这就是曼巴
05:44 有时候它效果不太好的一个原因
05:46 那就是说它除以这个图片序列的时候
05:49 可能会有些问题
05:50 也许呢它处理一个比如diffusion这样的问题
05:52 因为它就是一个序列
05:53 就是逐渐加噪音和逐渐去噪音的过程
05:55 这个可能还可以或者说处理一个视频对吧
05:58 因为一个视频就是一个序列
05:59 这样可能还可以
06:00 但处理一个图片呢
06:01 他可能就不是特别的在行了
06:03 好的
06:03 我们来看一看啊
06:04 就说呢我们有transformer对吧
06:06 还有我们的这个r n like这样的一个模型
06:09 就是比如说我的MANA
06:10 那transformer呢
06:11 就是把之前过去的每一个状态呢都给他记住
06:13 一个状态
06:14 一个状态都给他记住
06:15 那这样的话呢会导致他这个memory bank呢里面特别大
06:18 因为他记得序列糖了以后
06:20 他就把所有东西都放在里面啊
06:21 它就变成一个查询结构
06:23 它变得特别大
06:24 然后呢它变特别大的话呢
06:25 在JPU中呢
06:26 他也没法去有效的利用它的这个缓存呀
06:28 或者什么东西它呢就会导致它特别的慢
06:30 那么mama呢
06:31 它呢就是一个状态机的这样一个模式诶
06:33 所以它每一个状态建以后呢
06:35 就啊变成这样一个一个状态进行压缩
06:38 那它呢就会有一个非常好的一个啊
06:40 结果他呢就不需要像之前一样
06:42 你看他存了这么多东西
06:43 它呢就只要一直存一个状态就可以了
06:45 但是这个状态呢是可以展开的
06:47 它就可以存这么一个状态就可以了
06:49 好的
06:50 那么呢我们来看一看这个MANA这个类型
06:53 大概是什么样的
06:54 比如说JPT和MANA这样的东西呢
06:55 他就是说呢我呢是一个序列对吧
06:58 我比如说啊我想吃苹果
07:00 那么我出现我的时候呢
07:01 他预测下一个词
07:02 然后呢我就我下一个词是吃对吧
07:05 然后呢我下一个词是想
07:07 然后呢再遇到下一个词是吃
07:09 然后是平
07:10 然后是裹好
07:11 大概就是这样一个状态
07:12 就是他们每次都是去预测下一个词
07:14 那像bot和v i it这样的模型呢
07:16 就是他一次把所有东西都拿过来
07:18 然后呢直接去做这么一个
07:20 做一个这么一个全局的这样一个观察
07:22 他呢就是说每次预测下一个状态
07:24 每次预测下一个状态
07:25 每次预测下一个状态
07:26 要像LTM或者就是我们所说的mama这个样子
07:30 那就是这样一个结果
07:31 所以他呢也会有一个不同的这样的一个啊
07:34 情况吧
07:34 OK那我们来看一看这个啊
07:36 他们这个文章是怎么去做呢
07:38 他呢就是去模仿了微满马啊
07:40 就是我们进来以后呢
07:41 首先呢就说进来以后呢
07:43 就是一层一层的
07:44 它像CN一样
07:45 as次呢折半
07:46 然后呢这个通道数呢变多
07:48 大概就做这么一个内容
07:49 它呢和我们之前所说的spring transformer呢
07:51 基本上是一样的
07:52 它这个模式呢差不多
07:54 你可以去看看我之前讲spin transformer的视频
07:56 那这些这些模型呢我们之前都讲过曼巴呀
07:58 还spin transformer v曼巴呀
08:00 还有我们的V这个VM啊
08:02 什么都讲过啊
08:03 各种各样的模型
08:04 他呢就是用这样一个模型
08:05 然后看看最后的效果是什么样
08:07 能不能打败曼霸呢
08:07 于是呢
08:08 你看曼霸呢就是用来去做这种序列的模型
08:11 那如果要是我们做一个什么东西
08:12 是一个序列状态的诶
08:14 那my bu呢有个很好的结果
08:15 那如果要是图片的话
08:16 结果就不好
08:16 我们来看看最后图片的结果是怎么样的
08:18 那对一个低分辨率
08:19 一个图像的这样一个分辨任务呢
08:21 你看呢这个曼巴
08:22 各种各样的曼巴的这样的模型呢
08:24 它和这个曼巴out比起来呢
08:25 那它的效果就差了一些
08:27 你看im out呢是84.2
08:29 它呢只有83.9是吧
08:30 其实差的没有那么多啊
08:31 就差了一点
08:32 这边也是这样对吧
08:33 那么out呢就是一个把SSM直接扔掉
08:36 然后只有他没有SSM这个部分的
08:38 其他的这个模型
08:39 他呢发现呢效果呢还是不错的是吧
08:41 那效果呢就比妈妈的要好一点
08:43 其实对于低分辨率的这样一个
08:45 图像的分类任务呢
08:46 他呢就有一个非常好的一个结果
08:47 因为呢他呢就是MYA这个优势就体现不出来了
08:50 MYA就是对一个长的序列来进行一个分
08:52 离的这样一个任务吧
08:53 那对于一个高分辨率的图像分辨任务呢
08:55 他那优势呢就能够稍微体现一些
08:57 哎呀
08:58 那你看它的效果就比这个我们这个外卖out呢
09:00 要好一点
09:01 那因为呢这高分辨率这个图像呢
09:03 它需要对它就是它变成一个更像一个序列了吧
09:05 他需要记住更多的东西
09:07 那外卖out呢它里面只有CN的
09:08 他可能对于全局的这样的一个
09:10 这个记忆呢就不太好
09:11 那CN呢都是局部的
09:13 每一个地方都是一个局部的
09:14 比如说我们假如说这是一张图片的话
09:16 那CN的话就是把这按一下诶
09:17 这儿看一下
09:18 这儿看一下
09:19 左上角呢和右下角呢它就很难建立起关联来
09:21 但是my malt呢在my骂呢它就能够建立起联系来
09:24 所以MAMOUT呢在做这这种高分辨率图像分布
09:26 高任务的时候呢
09:27 他这个优势就不存在了
09:28 所以说呢这就是曼巴的这样一个特点
09:30 艾玛呢他就是有一个非常强的一个记忆能力
09:33 他的记忆能力也非常长
09:34 唉
09:34 所以说他呢就可以把这个图片上的
09:36 不同的位置呢给它进行一个关联
09:38 在进行关联了之后呢
09:39 效果就会变得非常好
09:40 那transformer呢也是这样
09:41 对
09:41 他也能够把这个上面的这个这个不同的位置呢
09:44 进行一个关联
09:44 那效果呢也可以达到一个非常不错的状态
09:47 好了
09:47 那这个视频呢就到这里了
09:49 感谢大家观看
09:49 我们下期再见
