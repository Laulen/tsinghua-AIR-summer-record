【博士Vlog】2024最新模型Mamba详解，Transformer已死，你想知道的都在这里了！
https://www.bilibili.com/video/BV1KH4y1W7cm/

00:00 大家好
00:00 我是PHTVLOG
00:02 那么之前的视频呢给大家讲曼巴的时候
00:04 讲的不是很清楚
00:05 然后呢
00:06 我最近和一个大佬讨论了一下模型的具体细节
00:09 现在呢对这个模型的具体细节呢
00:11 有一个更深入的了解
00:12 所以给大家重新去讲一下
00:14 玩把这篇文章
00:15 那玩把这篇文章讲的是什么呢
00:17 叫做linear time sequence modeling with ss
00:21 这样的一个模型
00:22 就是说它是使用了一个线性的时间的这样一个
00:25 序列去构建的这样一个模型
00:26 那么他的这个目的呢
00:28 就是去打败这个transformer
00:30 Transformer
00:30 它是一个二次时间复杂度的这样一个模型
00:33 那么我们首先先来回顾一下
00:35 就说我们之前有一个什么样的work呢
00:37 RN这个模型叫做这个循环神经网络
00:40 这个网络呢
00:41 它其实是1986年就发明的这样一个网络
00:44 我去查了下资料
00:45 发现这个网络真的是很久以前就发明出来了
00:47 他是怎么做的呢
00:48 就是它像一个拉锁一样在一个拉
00:50 它是一个拉链一样的拉锁上面去这个滑动
00:53 那么他就把这个这个每一次呢就是一个状态机
00:56 一个状态机
00:57 这样就把他们都给他装到自己
00:58 这个模型里面去了
00:59 所以我把它展开的话
01:00 它会变成这样的一个很长的这样的一个序列吧
01:03 所以说呢就是每次放进去一个
01:05 就像一个一个什么蛋白质的酶
01:08 在这个DNA上面去滑动的
01:10 这样的一个锁链一样的感觉
01:12 他的每一步都是一个诚信的
01:14 就是都是一个用乘法来做的
01:16 所以说呢它会产生一个问题
01:18 就是它的遗忘速度比较快
01:19 它的会因为他是诚信的嘛
01:21 还好后面的时候就会发生gradient vage
01:23 或者gradient exploding这样的问题
01:25 就是它的因为它乘来乘去乘乘乘乘乘乘乘乘
01:28 乘的结果就是它越乘越大或越乘越小
01:31 后面就后面就没法训练了
01:33 所以说呢他们只能做一个非常短的
01:35 这样的一个状态
01:37 还有一个事
01:37 就是他训练的时候呢
01:39 是这样一个一个拉锁去训练的
01:40 但test的时候呢它是可以一个一个这样并行去test
01:44 所以他训练的时候呢是相对比较慢的
01:46 但是它test的时候呢是相对比较快的
01:49 那我们来看一看在它之后有什么改进呢
01:51 就是改进就是LSTM这个东西呢
01:53 其实也是一个很久以前就出来的模型
01:55 是1997年就出来了这样一个模型
01:57 那么我们看RN呢
01:58 它就是一个澄清的这样的一个方法
02:00 就是它是都是乘法
02:02 然而那LSTM呢
02:03 它是一个加法的这样一个过程
02:05 所以说呢他呢有一个好处呢
02:07 就是它不会产生一个很强烈的这样一个
02:10 Gradient
02:10 vantage和gradient explosion的这样一个问题
02:12 就是他到后面的时候还能继续训练下去
02:15 它可以训练的比RN的更久一点
02:16 然后gr u这个模型呢
02:18 它其实就是LSTM指它稍微改进一下
02:21 你们的门的这个排列方式
02:22 让这个计算量呢变得更小一点
02:24 他们俩的其实是完全一样的这样的一个东西
02:27 那么这个RISTM它就引入了
02:29 比如说这个这个一些门啊
02:32 这些门的话呢
02:32 就可以让这个模型呢就有一个更长期的记忆
02:35 但是呢LSTM呢还是有同样的问题
02:38 和RN是完全一样的
02:39 它的训练的速度非常的慢
02:41 它开心的速度比较快
02:42 因为它是可以展开的
02:43 它呢也有一个问题
02:44 就是说当他序列很长的时候
02:46 他也会忘掉他以前的东西
02:48 所以说呢他也不能搞得太大啊
02:50 LSTM和RN都是这样
02:52 那么在2017年的时候呢
02:54 一个新的方法呢横空出世
02:55 这个方法呢叫做transformer
02:57 那么这个呢方法呢
02:58 它是使用这样一个多头注意力的这样一个机制
03:01 然后呢多头注意力这样一个机制
03:03 它是怎么去进行计算的呢
03:04 他就是算出来一个这样的一个二维的一个矩阵
03:06 然后在二维的矩阵里面呢
03:08 去计算这样的一个注意力
03:09 所以说在他春天的时候呢
03:11 它是很快的
03:11 因为它只要算每一个点的注意力就可以了
03:14 但胎心的时候
03:14 他要到这个矩阵中去进行一个对比
03:17 所以说泰森的时候他的速度是非常非常慢的啊
03:19 这是它不仅它速度慢
03:22 而且它存储的空间也要大
03:23 因为你要把这个矩阵存下来
03:24 所以他需要一个欧文方的
03:25 这样的一个巨大的这样的一个时间复杂度
03:28 所以它的这个它不仅复杂度高
03:31 它的这个这个存储的需求呢也非常的大
03:34 那么你看啊就是这个地方是这样的一个情况
03:36 所以说他是一个非常恐怖的占有过程
03:38 曼巴是一个线性的
03:39 所以说你能看到他们俩之间的这样的一个对比
03:41 是什么样子的
03:42 那么我们就来看看这个transformer
03:45 有一个什么样的结果呢
03:46 他的春节呢是比较快的
03:47 刚才讲过的是比较慢的
03:49 他们俩正好反过来
03:50 然后它的时间复杂度呢不仅时间复杂度
03:52 还有它的空间复杂度都是ON方这样一个级别
03:55 那么我们就要来讨论一下
03:57 就说在GPU中数据是怎么样去工作的呢
04:00 那么这个呢就是一个英伟达的
04:02 一个应该是A100的这样的一个GPU
04:05 那么这个GPU是什么样的呢
04:07 就是这些都是它的计算内核对吧
04:09 这它的计算内核
04:10 一堆一堆一堆的这样的一个计算线程
04:12 那么这个呢就是它的这个流处理单元对吧
04:15 一个一个的流处理单元哦
04:17 我刚刚去收了个快递啊
04:18 那我们继续来讲
04:19 就是说在GPU之中呢
04:22 它有很多的这样的计算和
04:23 这是它有它的小的计算和
04:25 所以GPU呢可以进行一个高强度的一个
04:27 并行处理
04:28 因为它有这么多的这样的一个计算核对吧
04:30 这些计算和可以把它们都计算出来
04:32 然后呢这个是他的l to catch
04:34 L to cash
04:35 就是他在这个芯片上面去自己做的这样的一个
04:38 集成的这样的一个存储单元
04:40 所以这个l to cash的速度是非常非常快的
04:43 然后呢在这边呢是它的内存控制器
04:45 对内存控制器之后呢
04:46 他接了一个叫HBM2的这样的一个东西
04:48 那这边也有这接的HBM2
04:50 HBM2呢就是它的片仔的这个内存啊
04:53 就是说它的L就是说存储的时候
04:56 是一个金字塔架构
04:57 它越是这个小的
04:59 这个就是越快的这个方式呢它越贵对吧
05:02 所以他做的呢只能越小
05:04 然后越大的这个存储方式它越便宜
05:07 但是它呢也就是速度比较慢嘛
05:09 所以说呢l to cash
05:10 就是它金字塔比较靠尖的地方
05:12 它的速度非常的快
05:14 但它的大小比较小
05:15 HEM2呢就是它的这个内存对吧
05:18 就像咱们的内存条一样的内存
05:20 普通的内存
05:20 那这个内存呢它呢比较的便宜
05:23 但是它呢就是说怎么说呢
05:25 速度呢比较的慢
05:26 并且还需要通过memory control
05:28 它就会有很高的延迟啊这一个事情
05:31 那么呢传送门呢有一个什么问题呢
05:33 它占的内存太大了
05:34 所以它只能在HBM2之中去运行
05:36 它就不可能在l two catch
05:38 上面进行一个比较有效的运算
05:39 所以说它的速度就比较慢
05:41 他每次的时候都没发heat
05:43 所以说只能跑到HBM里再去算一遍
05:46 那么这个就是我们transformer的这样一个问题
05:49 我们给大家讲过了
05:50 那么我们才想想
05:51 就说2017年之后呢
05:53 大家就想办法就说去改进transformer
05:55 或者是改进LSTMRN去造一个新的模型
05:58 这个模型呢它的目标是什么呢
06:00 就是他春天的时候呢非常的快
06:02 拍戏的时候他也非常的快
06:04 然后呢他这个有没有这个额外的问题呢
06:06 他没有额外的问题
06:07 我希望他的ram和time的时间复杂度
06:09 都能达到这样一个级别
06:11 那这样的话就非常好地解决了
06:13 我们现在的这个呃遇到的一些瓶颈的问题
06:16 那么呢有什么样的方法去解决这样的问题呢
06:19 有一个方法呢叫做SSM
06:21 就是一个状态机的这样的一个方法
06:23 那状态机呢就是像我们去做的这样的一个
06:25 递归一样
06:26 你看我们在出地扣的时候
06:28 这个其实叫递归
06:29 对不对
06:29 就说这是上一个时间的
06:31 上一个时间部的这个情况
06:33 这是下一个时间步的情况
06:34 下一个时间步呢等于上一个时间步呢
06:36 乘以一个A再加上另外一个函数对吧
06:39 这个呢就是一个状态机的这样的一个过程
06:42 那就是它是一个递归的这样一个过程对吧
06:44 那么这个传说嘛呢
06:44 它是计算一个二维的这样一个矩阵
06:46 计算一个二维的矩阵呢是非常慢的
06:48 因为他把所有的时间都给它保存下来
06:50 状态机呢只需要保存当前的这样的一个状态
06:53 时间就可以了
06:54 他不需要去保存太多的这样的一个内容
06:56 那么呢这个使用的它里面是个A呢
06:59 就是说啊如果你要是就随便取一个值A
07:02 那么它的结果不好
07:03 所以A呢最好是一个动态的这样的一个东西
07:06 那么A呢使用一个叫ho matrix的方法去计算
07:09 那么这样的话就会有一个更好的一个效果
07:11 然后呢黑方matrix你发现了它也是个二维的
07:14 所以说他就跟穿梭装没区别了
07:15 他也是个二维的矩阵
07:17 那怎么办呢
07:17 就是我们能不能把A给它变成一个一维的
07:20 是可以的
07:20 通过一个像SVD一样的算法
07:22 就可以把这个A呢给他直接压成一个一维的
07:24 就是说相当于提取它的一个主成分
07:26 然后呢把它压成一个一维的
07:28 通过这样的一个方法
07:29 那么这个就是曼巴的这个主要的
07:31 这个使用的方式
07:32 当然曼巴并不是第一个做这个的
07:34 之前还有SSM就是他之前的
07:37 所以曼巴是改进了SSM
07:38 解决SSM里面一些不能解决的问题
07:41 那么他慢慢能不能进行这个parallel computer呢
07:45 也是可以的
07:45 也就是说以前的时候大家都是这种线性的
07:47 这种一个一个的去计算的这样一个模式
07:50 但在显卡之中呢
07:51 我们要想算
07:52 就是说这个叫做累加和的话
07:54 它是可以通过这样的一个方法
07:56 还叫叫做parallel reduction这样一个方法去计算
07:59 这个好像不是reduction
08:01 是另外一个词
08:02 但是我忘了
08:02 就是说它可以通过这样的一个方法去计算啊
08:05 我们之前在视频中也讲过
08:06 就是说他计算的方法是啊
08:08 你看它是一个
08:09 它是一个这样的一个像蝶形运算的运算
08:11 就可以把每一步的累加和都算出来
08:13 比如上面的是一堆数
08:14 对不对
08:15 这个就是前两个数的加法和
08:17 这个呢就是前三个数的加法和
08:19 这个呢就是前四个数的加法和
08:20 这样意思类推下去
08:22 然后呢还有一个事儿呢
08:23 就是说我们刚才说了嘛
08:25 就是说我们的带如果在我们的JPU之中的话
08:28 它有cash
08:29 还有内存
08:29 如果是我们使用cash的话
08:31 它速度比较快
08:32 如果使用内存的话
08:32 那它速度就比较慢
08:34 那么怎么去避免
08:35 就是说在这两个东西之间来回来去的copy
08:38 或者说就是它产生一个很长的一个延迟
08:41 那么最开始的方法就是说我们在drum和ram之间
08:44 来回的倒腾对吧
08:45 这就是传former的方法
08:46 因为它没有办法
08:47 它的内存用太多了
08:48 所以这个方法非常的慢
08:50 那怎么办呢
08:51 我就是能不能在d ram里面去算额
08:54 再从d ram里面把数据copy到SRAM
08:56 在ram里面把它全部都算完
08:57 算完了之后呢
08:58 再给他直接copy回d ram
09:00 相当于只有一次过程就可以解决这个问题
09:02 那么这个妈妈呢就把这个问题给解决掉了
09:06 那么在SM之中呢
09:07 其实还有一个问题
09:08 就是有的时候呢
09:09 他呢这个这个数据呢还是会产生比较大
09:12 产生比较大的
09:12 在ram之中就没办法都存下来
09:14 怎么办呢
09:15 GPU啊
09:16 它的计算呢其实是比它的内存的要快的
09:19 所以说怎么办呢
09:19 我就在CPU中呢再把它重新算
09:21 把它重新算一遍
09:22 这样我就可以保证所有的数据呢
09:24 还依然保存在ram里面
09:25 那些巨大的数据呢
09:26 我们并不需要再去提取出一遍了
09:29 那这样的话呢就可以继续增加它的速度
09:31 那么就是这边就是这个ma的这样一个模型
09:34 ma这个模型中呢
09:35 它有些地方是重新去进行这个计算
09:38 是它进行一些反复的计算
09:39 他这样做虽然会增加功耗
09:41 虽然会可能会增加时间
09:43 但是呢它总比从内存里读一遍可能要快一点
09:45 所以他是使用了这样的一个方法
09:47 那么这就是所谓的弯把的这样的一个方法
09:49 叫硬件感知算法
09:50 那么慢慢的就成功的把问题解决掉
09:53 也就是说呢我们呢该在HTBM2里的东西呢
09:55 还在HBM2里面
09:56 但是当我们计算计算的时候
09:58 他都是在高速的L2cs里面
10:00 它就和这个和一个超高速的一个交互
10:03 那么它总比在HBM2里面进行交互的
10:05 这个方法要好
10:06 所以说呢这就是所说的这个曼巴的
10:08 这样一个核心算法啊
10:10 这就是满满的一个算法
10:11 如果把它直接画出来的话
10:12 是这样
10:13 这是他论文中的算法
10:14 所以说呢这个呢是金字塔比较尖锐的地方
10:16 他的size size比较小
10:18 但它速度比较快
10:19 这个是他金字塔比较下面的地方
10:20 它size比较大
10:21 但它速度比较慢
10:22 所以说呢我们呢就是尽可能的
10:25 把我们所有的核心运算都给它放到这个ram里面
10:27 这样的话就可以让这个系统的变得更快
10:29 那么也是因为这个SSM这个方法
10:32 这个方法呢它非常的SM
10:34 这个方法它非常的省内存
10:37 所以说他才可以做这样一个优化
10:39 那么transformer呢
10:40 他根本就不可能做这样一个优化
10:41 因为它根本就不省内存
10:42 OK那我们来看一看这个mama
10:45 还有一些其他的算法是什么呢
10:46 就是说呃在这个状态机状
10:49 在这个连续状态机里面copy一个东西
10:52 它copy的时候整个copy过去呢是比较容易的
10:55 但是呢当它中间有一些空档的时候
10:57 我们不想要这个空档
10:58 我想copy的时候呢
10:59 把中间进行一个有选择性的一个copy的话
11:02 都会比较难
11:02 慢慢呢他呢就设计了一个算法
11:04 把这个问题解决掉
11:05 以前的是SSM
11:06 这个不是他发明的对吧
11:08 他设计的他发明的东西是那个selection的部分
11:10 这是他发明的这个部分
11:12 那么呢我们现在就把这个表都填完了对吧
11:14 妈妈呢
11:15 我们也不需要特别去了解他到底是怎么做
11:17 因为它其实和transformer的是完全一样的
11:19 它的它的这个这个可以完全进行一个替代对吧
11:23 所以说呢我们并不需要去知道斑马
11:25 它有一个最后一个什么样的一个结果
11:26 我们有时候看看麦霸的效果是怎么样
11:28 还有它的速度是怎么样的
11:29 那MANA呢它的training的phrase呢就非常的快
11:32 他的test phrase呢也非常的快
11:34 那么它的没有额外的问题
11:35 他的ram和time的时间都是这样的一个时间
11:39 那么所以说呢拆GBT呢以前会卡对吧
11:41 那如果他用上这样的新技术的话
11:43 可能就不会卡了
11:44 因为它test的部分非常的慢
11:46 但是现在test的速度可以非常快
11:48 可以的也更节省他的这样一个计算资源
11:50 那么就来看看曼巴
11:51 这样的一个情况是怎么样的呢
11:53 这个呢是一个传说面上的一个模型对吧
11:55 它的大小呢也是2.8个BD
11:56 就是曼巴它呢也是2.8克
11:58 他们俩参数量是完全一样的时候呢会怎么样呢
12:01 他在训练的时候
12:02 训练在一个2K的大小的样本上面
12:04 所以这个样本的大小呢就是2K
12:06 就是从这个地方截断了
12:07 到后面就没有东西了对吧
12:09 所以你看attention
12:10 这个这个transformer的self attention
12:12 在超过他的这个这个范围之外呢
12:15 很快就进入到了一个不收敛的这样一个状态了
12:18 但是曼巴你看它因为它是一个状态机嘛
12:20 它是一步倒一步一步倒一步
12:21 这样一个递归的一个方程
12:22 它就可以一直持续一个很长的时间
12:24 直到这个后面
12:25 它只是进行了一个略微的一个上升
12:27 所以说曼巴呢它可以去进行一个更长的有效的
12:30 这样一个文本处理
12:31 那么他可能以后也能进行一个视频的处理
12:34 因为分论和视频其实是很相似的
12:36 那么我们在分享这篇文章之后呢
12:38 再给大家讲讲vs mvc m
12:40 就说MANA里面他没有做太多的这样的一个对比
12:43 他没有画出图来
12:44 所以说呢我们有时候看着呢就不是很清楚
12:47 但是呢为什么呢
12:48 就把他这个图呢给它画出来
12:49 进行一个比较好的对比
12:51 你看啊这个速度非常快
12:52 这个文章一个月就出来了
12:53 那么呢我们来看
12:54 就说你看他在这个上面是VM对吧
12:57 这个是它的vision版本
12:58 这个是传错嘛
13:00 能看到的就是说他在做这个classification的时候
13:02 效果呢好对啊
13:04 这个是他的
13:04 就说做那个呃框那个车
13:07 比如框个车对吧
13:08 框个人对吧
13:09 就是把这个人从这个图片里标出来
13:11 IMU呢就是他说你原来有一个ground truth
13:15 ground truth就是那个真实的那个值
13:17 那真实的值和你新的那个值呢
13:19 它中间的对吧
13:20 它能够有多大的重合面积
13:22 综合面积越高
13:22 那说明它的效果越好对吧
13:24 所以i mu是做这样一件事
13:25 你看它效果也更好
13:26 但是啊这个图有点误导
13:28 为什么呢
13:28 它是从71开始到77
13:30 所以它其实只提升了几个点
13:32 你看着好像很厉害
13:33 但其实它就提升了一点对吧
13:35 这个也是提升了一点
13:36 提升了2%对吧
13:37 这个提升了2%
13:39 这个人可能提升了一点
13:40 但是你要知道
13:41 transformer已经是个很牛的模型了
13:42 所以说提升2%呢
13:44 也是一个非常大的一个改进吧
13:46 只能这么说
13:46 那么我们来看这个是它的速度的这样的
13:49 一个对比
13:49 那速度对比的话
13:50 你可以看到
13:51 就说曼巴的速度是
13:52 比这个这个在transformer上的这样模型呢
13:55 就快了2.8倍对吧
13:57 它越大呢就是越快对吧
13:59 所以说当后的RESUDO更大的时候
14:01 它变得更快
14:01 还有一件事就是resolution呢
14:03 就是说当他的这个分辨率越高的时候呢
14:05 你占的内存也是越大
14:06 因为你想嘛你做视频也是这样
14:08 分辨率越高
14:09 它的速度越慢对吧
14:10 渲染的时间速度越慢
14:11 然后它的这个干这种资源的也越多
14:13 那么你看MANA的资源
14:15 是一个线性占用的这样一个状况
14:17 但是呢这个transformer它是一个平方占用
14:19 所以到后面就已经out of memory了
14:21 就是它的memory直接就爆炸了对吧
14:23 所以这个地方它可以减掉86%的memory
14:26 所以它呢可以去处理更大的这样的一个数据
14:28 比如说闲的时候拆GBT的可能只能处理一个
14:31 比如说5000大小的一个数据
14:33 那么现在呢它可以处理5万大小的数据
14:35 那么拆JBT呢就可以直接生成一本小说
14:37 他们就脱离了之前那个就说生成一本小说
14:40 到后面他已经忘了前面写什么的
14:42 这样的一个窘境对吧
14:43 他就可以到后面一直持续的生存下去
14:45 所以呢如果这个技术被OpenAI用上的话
14:48 应该是一个非常爆炸性的结果
14:50 那么我们就来看就说vision mm呢
14:52 这个其实呢是和这个VIT呢是非常像的
14:56 就是vision transformer
14:57 vision transformer呢他就把一个图片切成一块一块
15:00 然后塞到里面去
15:01 然后呢就用他的这个self attention的这样一个机制
15:03 得到一个最终的输出
15:04 那微信密码呢其实是一样的
15:06 它也是把一个图片对吧
15:07 切成一块一块的
15:08 然后呢放到这个这个一个模型里面
15:11 然后之后做出一个这样的一个结果
15:13 然后呢这个是传former的这样的一个模型对吧
15:15 那么这个是妈妈的一个模型
15:17 所以它是非常相似的啊
15:18 这就是说呢就是说从传错门到微针传错门
15:22 那么transformer是2017年出的位置
15:24 PSER是2020年出
15:25 他用了3年的时间
15:26 但是ma呢是2024年
15:28 应该是1月份出的
15:29 然后这个东西呢是2024年2月份出的
15:31 所以他们只用了大概一个月的时间呢
15:33 就把它给搞出来了
15:34 但这样做呢也这样对比
15:36 其实也不是特别公平
15:37 因为这个是一个新的idea
15:38 他是从来没有人去做过的
15:40 VNASA是一个new的一个idea
15:43 是一个完全新的一个idea
15:44 但是呢这个ma呢它呢就是怎么说呢
15:47 是和上面这个呢是一种
15:48 是一种增就递增的一种关系
15:51 它是就是基于这样一个工作
15:53 做了一个新的模型的这样的工作
15:55 所以说呢他呢其实并没有发明出什么
15:58 太多新的东西
15:59 不像是微针传former
16:00 它是整个从头发明了一个新的东西
16:03 所以说他这个时间对比呢
16:04 也不是一个特别特别有有这个意义的
16:07 一个对比啊
16:07 我们就大概知道就是他这个work呢确实出来的快
16:11 说现在CV呢有多卷呢
16:12 你看当时2017年的时候呢
16:14 3年才卷出来一个东西对吧
16:15 那现在的话呢你要是不能用一个月
16:18 两个月的时间把一个东西转出来的话
16:20 你就被人超越了
16:21 所以这个现在做微针呢
16:22 真的是非常难做的一件事情啊
16:24 也推荐大家不要入CV这个坑
16:26 这个坑的话非常的恐怖
16:28 那我们来看看它的效果是怎么样的呢
16:30 首先看到reset这样一个模型
16:31 我们之前讲过reset
16:33 reset这样一个模型去进行对比
16:34 就是convolution的这样的一个啊
16:36 就是卷积神经网络的这样一个模型
16:38 然后呢还要去把它和传送门进行对比
16:41 就是和变压器这个模型进行对比
16:42 那就是传送门的这个模型
16:44 然后这个底下的就是他的mama的这个模型
16:46 就是SSM的这样一个模型对吧
16:47 这是可能是别的SM
16:49 这个ma是他自己的一个SSM
16:51 然后就能看到
16:52 就说首先这件事呢我们要看这image size是多少呢
16:55 对吧
16:56 你看这image size大大分都是224的平方对吧
16:59 都是224224这样一个size
17:01 那这个VIT呢
17:02 可能用了一个更高的这样的一个值对吧
17:04 因为它是一个更大的一个模型
17:06 那L14的话也是224
17:08 但L16的话就变得更大一点
17:10 然后呢你看它的参数量呢
17:12 你看这些参数量呢
17:13 其实CN的参数量还是挺省的对吧
17:15 这是不是很大的一个参数量
17:17 然后呢到了这个传former这边呢
17:19 它的参数量变得非常的恐怖了
17:21 然后呢到了这个MANA这个地方呢
17:22 它的参数量是比较小的
17:24 你看那个七兆的话呢
17:25 可能就是大概它比这个CN的所有的都小对吧
17:28 那他可能和这个这个差不多是一个size
17:30 但你要去对比啊
17:31 就是七兆和六兆的这个模型呢
17:33 你看它的模型呢
17:34 有78.3%的这样一个准确率
17:37 算只有72.2%的这样一个准确率
17:39 所以它的准确率呢其实是比较高的
17:41 然后呢你看这26兆的这样一个模型呢
17:43 对于他来说的话
17:44 26兆的模型呢它就提高了一点对吧
17:46 它会提高大概1.1%的这样一个效果
17:49 但是呢它的参数量呢
17:50 但是你不能去只对比它的参数量
17:52 因为呢它的使用机制也不一样
17:54 它们可以用更多的这样的一个
17:56 高速的这样一个缓存
17:57 所以说它的速度会比他快很多
18:00 你看这个地方它快了三倍的速度
18:01 大概是这样的一个情况啊
18:03 这是它的内存的这样一个消耗
18:05 那么呢我们来看一看
18:07 他最后做出来的结果是什么样的呢
18:08 这个是mask r cn这样一个方法
18:11 mask r ca呢
18:12 他可以把这个图片之中的飞机呢给他标出来
18:14 那你去看这个飞机啊
18:15 这个地方应该是这个地方
18:17 应该是它的飞机的机翼
18:18 可以把他的飞机呢给它
18:19 比较有效的给它标出来对吧
18:21 那么曼巴呢也可以把这个飞机呢
18:23 比较有效的标出来
18:24 但是你看这个机的部分呢
18:25 他就没有标出来对吧
18:26 他只知道飞机的主体标出来
18:28 可能跟他的tension机制是有关的
18:30 但是呢对于这个模型来说呢
18:32 这个图片太大了
18:32 它根本连标都标不动对吧
18:34 这根本就是做都做不了
18:36 所以就直接把原图给你输入出来
18:38 根本就没有标出来他飞机的部分
18:40 所以说妈妈呢可以比较有效的
18:41 把这个可以去有效的去处理
18:43 一个比较大的这样的一个size
18:45 你看他可以去capture very large object in the image
18:48 可以去图片中
18:49 找到一个非常大的这样的一个物体
18:51 那么呢我们介绍完了这个微信版本之后呢
18:53 给大家来稍微看一下U盘
18:55 把这样的一个因为妈妈这样的一个工作
18:57 是做在医学上面的这样的一个工作
19:00 那么医学上面的工作呢
19:01 他就是说呃
19:02 就是他使用了一个像unit一样的东西对吧
19:05 这个东西很像一个unit unit
19:07 它是做什么的呢
19:08 就是比如说我输入一张图片对吧
19:10 输出一张图片
19:11 输入的图片是卫星的图片
19:12 输出的图片是一个地图的图片
19:14 我就可以把这个卫星图片呢
19:15 给它转变成一个地图的图片
19:17 所以说呢我们就比如说这个
19:19 大家这个使用的地图
19:20 比如google地图或者百度地图
19:22 它肯定是使用一些这样的技术
19:23 他不会去人工的去把那个路都画一遍
19:25 那画一遍画太累了
19:27 所以他可能就使用一些这种机器标注的这种方
19:29 法去把这个路呢都给它生成出来
19:31 这样的话呢就节省大量的人力成本嘛
19:34 然后人只要去check一下
19:35 只要去检查一下这个路是不是正确的就可以了
19:37 这样的话呢非常的方便
19:39 那么他要做什么事呢
19:40 比如说我们输入一张图片
19:41 是一张有雾的图片
19:42 然后呢输出的图片是一张没有雾的图片
19:45 那么我们就可以做一个去雾镜对吧
19:47 那我还可以输出一个人的图片
19:48 输出一个人的图片
19:49 输出一个人美颜的图片
19:51 做做一个美颜美颜的这样一个效果好
19:54 那么他做出来他做了一个什么事儿呢
19:56 就是他输入的时候呢
19:57 是一个骨头的这样的一个照片啊
19:59 啊这可能是个腹部的这样的一个照片吧
20:01 然后他输出的时候
20:02 就把这个腹部里的器官
20:03 都给你进行了一个标注好
20:05 那么他输入的时候是这样的
20:06 输入的时候是这样这样去训练
20:08 那么那你在赛季上新的图片的话
20:10 他做test的时候呢
20:11 它也会变成这样的一个情况
20:12 这就是unit这样一个大概的一个思路
20:15 那么你就可以看到就是说在不同的方法上
20:18 这是光truth
20:18 就是这是真实的一个结果
20:20 就是说人去标注出来的结果
20:22 这个呢是使用了不同的
20:24 这样的去做了这样一个结果
20:25 他们会看到就说他拿箭头指出来的地方是Knight
20:29 就说呃可能会不好的这样一个位置
20:31 你看这个地方就会把肝的这个位置呢
20:33 给他装到了胃的这个位置对吧
20:35 所以他就产生了一个不太好的这样的一个图形
20:38 你看有的地方它就不能正确的进行一个分类
20:40 但是呢你看1万8呢可以进行一个有效的分类
20:43 你看这个图也是这样
20:44 这个地方缺了一块
20:45 这个地方是完整的
20:46 而这个地方呢也是
20:47 这个地方呢本来应该没有东西对吧
20:49 他那多的一个器官
20:50 那么U弯骂也没有问题
20:52 但这个地方也是这样
20:53 也是就说错误的分类
20:54 但是呢U万八呢
20:55 可以比较正确的去进行一个分类
20:57 然后呢我们就来讨论一下
20:59 就说这个东西对我们以后的科研
21:01 会有一个什么样的影响呢
21:02 那么呢首先呢第一件事就是说传送门和曼巴的
21:06 他们其实实际上是几乎完全一样的东西
21:08 他们的东西呢几乎是一样的
21:10 所以说呢他们可以进行一个直接的替代
21:12 就是说把妈妈直接就替代成传错门就可以了啊
21:15 唯一的不同就是用模型内部的这样的一个东西
21:18 不同
21:18 它的输入和输出呢都是一样的
21:20 所以说他们之间是属于一个可以互相快速期待
21:23 这样一个过程
21:23 所以说呢以前的时候呢
21:25 我们用传送门来做的
21:26 任何东西都可以用买马来代替
21:28 比如说这个是传送门做的这样一个模型对吧
21:31 它的左边的编码器呢变成了BT
21:33 然后它的解码器变成GBT
21:34 那么JBT呢以后可能变成了JPM
21:36 因为呢它可以使用源码来代替
21:38 它里面的一些过程
21:39 然后呢我们来做的这个clip这样一个模型呢
21:42 我也是在之前讲过
21:43 还有text encoder
21:44 这是一个VT或者是一个RISNE
21:47 这是一个VIP
21:48 不是这是一个transformer
21:50 这是一个transformer
21:50 因为它是文本
21:51 然后呢你现在可以用买来代替这个地方呢
21:54 它可以是rise style或者是一个transformer
21:56 那现在可以用买来代替
21:57 那看看那transformer
21:59 transformer的效果是比rs要好
22:01 MANA的效果比transformer可能要好
22:03 速度主要是速度要快
22:04 所以说呢当时的时候这个OpenAI
22:06 可能用了几百块显卡来训练这个东西
22:09 现在呢可能还用几百块显卡
22:10 它能够得到一个比以前更好的效果
22:12 所以说呢可能以后clip呢会出现一个新的版本
22:15 可能会出现一个version two
22:16 version two的话呢
22:17 可能就是用ma来代替里面的东西
22:19 可能会得到一个比现在更好的效果
22:21 然后OpenAI这个公司呢
22:23 你也知道他这个公司呢比较疯狂对吧
22:25 做什么东西呢都做个顶流
22:26 做个视频生成的
22:27 做个骚扰啊
22:28 做个做这个生成的时候
22:30 以前是用干他用那个diffusion
22:32 然后呢以前的时候大家都是去用这个类来进行
22:35 类比的话呢
22:36 他直接给你整了一个这种对比学习
22:38 所以说呢这个OpenAI呢做什么东西做的比较疯狂
22:41 这个这个东西咱们是做不了的
22:43 因为他用了400个mini的size
22:45 也就是说四个亿这样的图片来做的
22:47 那么咱们哪里去搞四个亿这样的这样的图片
22:50 文本对咱们都存不下
22:51 对不对
22:52 那他们可以把这些东西都存在内存里
22:54 所以说他们的资源是非常丰富的
22:55 那么他们如果现在
22:56 如果是使用ma这样的模型的话
22:58 使用的这个显存资源的更少
23:00 所以说呢也许他们会在之后做一个
23:03 更大size的这样一个东西
23:04 尤其现在common crow呢
23:05 现在做的其实现在已经有100
23:07 就是以前是40个
23:09 可能有100个media或者是200个media这样的
23:12 所以他们可以用更多的这样的数据
23:14 去生成一个更好的结果
23:16 那么我们就期待open i
23:18 给我们带来一个什么新的大活吧
23:20 然后呢这个呢就是微针传former可以改成MMSUMER
23:24 所以说呢呃所有的用transformer所做的事情
23:27 都可以改成满满
23:29 并且呢以前呢说transformer呢它有一个问题
23:31 他size比较大
23:33 所以说呢他在处理图片的时候
23:35 可能就已经到了他的一个比较极限的
23:36 一个状态了
23:37 那么呢因为他的这个rap呢
23:39 还有他的这个speed呢都发生了一个变化
23:41 所以说呢在mama上面呢
23:43 也许我们就可以去直接去处理一个视频
23:45 你看这个地方呢他是传送门去处理一个视频
23:47 那这个模型呢它没有太火起来的原因呢
23:49 就是它的计算量实在是太大了
23:51 但是呢用传送门的话
23:53 要用改成妈妈的话呢
23:55 他以后生成这个视频啊
23:56 或者是什么样的这个过程呢
23:57 就可以直接去做
23:58 那么带来一些新的可能性
24:01 因为它的速度变得更快了
24:02 所以说呢这个处理视频呢
24:04 可能就变成了一个未来可能的一件事情
24:07 并且呢还有一件事
24:08 就是说他能做到一个实时处理的这样一个过程
24:10 因为它的速度变快了
24:11 当我们使用多款显卡的时候
24:13 可能这个视频呢就不能变得实时处理啊
24:15 那么这样的话呢
24:16 我们就可以有更多的这样的一个应用了对吧
24:18 那么还有一个事儿呢
24:19 就是说比如说我们这个现在有这个汽车
24:22 汽车上面会有一个激光雷达
24:24 激光雷达可以拍到周围的这样的一个物体
24:26 那么有些车呢现在就有激光雷达
24:28 然后有的车呢是通过摄像头的方法
24:30 那么不管你是摄像头还是激光雷达
24:33 它呢都需要去辨别周围的这样的一个物体
24:35 的这样一个情况
24:36 那么使用一些简单的算法
24:38 有时候会有不好的效果
24:39 或者说呢它产生一些安全问题
24:41 那么可能使用这样的一个transformer
24:43 这样一个模型呢
24:43 再加上一个可解释性的
24:45 因为NP可解释性也会比transformer好一点
24:47 所以说它呢会有一个更好的可解释性的
24:50 这样一个结果
24:51 那么如果它有可解释性比较好的话呢
24:53 呃也许在自动驾驶上面呢
24:54 以后会用到这样一个东西
24:56 并且呢以这个骂这个模型呢
24:58 以后可能会彻底改变GPU的架构
25:00 因为如果大家要是都去使用这样的东西的话
25:02 那么以后英伟达公司或者什么高考
25:04 这样的公司呢
25:05 他可能会设计一个架构
25:06 专门就是去feat ma这样一个模型
25:08 那么它的速度也会变得更快一点啊
25:11 我说的妈妈可以直接去处理一个视频
25:13 这样的一个情况
25:14 如果要能直接去处理一个视频呢
25:16 那么呢也许以后会有更多的可能性
25:18 因为我们要知道在deep learning中啊
25:20 我们去进行人为干预越多呢
25:22 有时候它的效果就变得越好
25:24 会变得越差
25:24 所以说有的时候呢我们就是无为而治
25:26 用一个这种人工干预比较少的这样的一个方式
25:29 它也许会有更好的一个效果
25:31 所以我们如果要是人工的切成一个frame
25:34 一个frame这样的结果
25:35 我们用一个什么流啊
25:36 双流网络呀
25:37 或者使用一个像CN的这样一个
25:39 3DCN这样一个网络呀
25:40 它有时候效果会变得比我们去一个一个出去
25:42 frame更好
25:43 因为它能加强他frame之间的这样理解嘛
25:46 所以说如果要是慢慢能够
25:47 去整个处理一个视频的话
25:48 可能会比我们人工去切成一帧一帧的
25:50 这样效果可能要好一些
25:52 那么这就是我今天所讲的所有内容了
25:54 那感谢大家观看
25:55 我们今天就把mama给大家进行一个完整的梳理
25:58 想念曼巴
25:59 还有它的一些应用对吧
26:00 比如说微信MANA
26:01 还有U曼巴
26:02 没有想太多的这个曼巴的这个相关的应用
26:04 还有现在已经涌现出了很多新的这个应用
26:07 如果以后呢我们也觉得有必要的话
26:09 会给大家继续的去深入的探讨一下
26:10 妈妈的这些模型
26:11 那也感谢大家的观看
26:13 我们下期再见
